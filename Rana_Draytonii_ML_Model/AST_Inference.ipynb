{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To Do:\n",
        "automatically detect and adjust if cpu or gpu is available\n"
      ],
      "metadata": {
        "id": "FTqOQqmEefbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0. How this works\n",
        "\n",
        "This is a Jupyter Notebook file, running in google Colab. that means we get to use their could for our processing power, and also get to do some nice things like connect directly to google drive.\n",
        "\n",
        "Jupyter Notebook files let you put things like this text box in line with your code, and also execute code in various orders. For this, all we want to do is go through the cells, top to bottom, executing each code cell by clicking the play button in the top left corner. it will have a stop button available while executing then a green check when finished. The output of a given cell will be below the cell, and might give some updates on the progress of execution.\n",
        "\n",
        "There will also be a cell as your are going down where you will have the option to alter some inputs. This mostly amounts to giving the path to your \"Rana_Draytonii_ML_Model\" folder and the name for your output excel file.\n",
        "\n",
        "Everything you need to run predictions is contained in my github repositiory, which will be cloned into this notebook in one of the next cells, and the google drive folder \"Rana_Draytonii_ML_Model\". You can put this folder wherever you want in your drive. It will have files in it as such:\n",
        "\n",
        "Always/before you run anything:\n",
        "\n",
        "*   [Folder] Wav_Files_Input: Where you place files you want processed\n",
        "*   [File] best_audio_model1.pth: My pretrained weights. leave this be\n",
        "*   [File] labels.csv: Labels for the model, also let be\n",
        "*   [File] AST_Inference: thats this file, just wanted everything in one spot\n",
        "\n",
        "\n",
        "\n",
        "During running it will create:\n",
        "\n",
        "*   [Folder] temp_file_storage: used in script, doesnt matter outside of while executing these cells\n",
        "*   [Folder] ResampledAudio: similar to above\n",
        "*   [File] whatever_you_name_it.xclx: this is the excel file where your results will be stored after everything executes!\n",
        "\n",
        "Before running any cells, just make sure you have that \"Rana_Draytonii_ML_Model\" folder in your google drive, and you have the sign in to that drive. One of the next cells will ask you to sign in and give access to your drive when you run it, which will allow this notebook to access those files directly from your drive. I (Tyler Schwenk) will not have access to your drive, this should all be running on your own account. Also make sure you have moved the files you want reviewed into the \"Wav_Files_input\" folder within that folder (maybe start with an hour and a half or less length of audio at a time [refer to step 3]).\n",
        "\n",
        "Once those few things are set up, youre ready to roll. Just scroll down and read the text cells and run the code cells, one at a time.\n",
        "If you have any questions reach out to me at tylerschwenk1@yahoo.com or ask good ol google.\n",
        "\n",
        "Happy Frogging\n",
        "\n"
      ],
      "metadata": {
        "id": "bCRnLuJXd19p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAp82daib2nA"
      },
      "source": [
        "## Step 1. Install and import required packages.\n",
        "\n",
        "These will install onto this instance of google colab. it will not be installed to your computer, and so when you reload this notebook it will need to be installed again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQbQfrkjatF0",
        "outputId": "21287f39-164f-48de-8f4c-fd0b70601651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Cloning into 'ast-Rana-Draytonii'...\n",
            "remote: Enumerating objects: 789, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 789 (delta 141), reused 188 (delta 98), pack-reused 503\u001b[K\n",
            "Receiving objects: 100% (789/789), 2.48 MiB | 3.06 MiB/s, done.\n",
            "Resolving deltas: 100% (401/401), done.\n",
            "/content/ast-Rana-Draytonii\n",
            "Collecting timm==0.4.5\n",
            "  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.5) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.5) (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.5) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.5) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.5) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.5) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.5) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.5) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.5) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.5) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.5) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.4.5) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.5) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.5\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=ee5b90f12bd7187660092ea435a62855b208db9a891fde6e49a735e7da147368\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !git clone https://github.com/Tyler-Schwenk/ast-Rana-Draytonii.git\n",
        "    sys.path.append('./ast-Rana-Draytonii')\n",
        "%cd /content/ast-Rana-Draytonii/\n",
        "\n",
        "! pip install timm==0.4.5\n",
        "! pip install wget\n",
        "! pip install pydub\n",
        "import os, csv, argparse, wget\n",
        "os.environ['TORCH_HOME'] = '/content/ast-Rana-Draytonii/pretrained_models'\n",
        "if os.path.exists('/content/ast-Rana-Draytonii/pretrained_models') == False:\n",
        "  os.mkdir('/content/ast-Rana-Draytonii/pretrained_models')\n",
        "import torch, torchaudio, timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.cuda.amp import autocast\n",
        "import IPython\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "from pydub import AudioSegment\n",
        "from src.models import ASTModel\n",
        "import json\n",
        "from pydub.utils import mediainfo # for retrieving metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PEQh5p_cNqJ"
      },
      "source": [
        "**Mount google drive where weights are stored**\n",
        " (requires authentication)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExLSwOaXcJeu",
        "outputId": "46f655ee-e42e-4f35-83c5-3cd8c7e91c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess audio files**"
      ],
      "metadata": {
        "id": "H4nCR_vTxkqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First split into 10 second segments"
      ],
      "metadata": {
        "id": "Iw8G1x9Xzdnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def split_all_audio_files(input_dir, output_dir, segment_length_sec=10):\n",
        "    # Make sure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loop over all files in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        # Check if the file is a .wav file\n",
        "        if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "            try:\n",
        "                # Full path to the original audio file\n",
        "                audio_path = os.path.join(input_dir, filename)\n",
        "                # Load the audio file\n",
        "                waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "                # Calculate number of samples in segment_length_sec\n",
        "                num_samples_segment = segment_length_sec * sample_rate\n",
        "\n",
        "                # Calculate total number of segments\n",
        "                total_segments = math.ceil(waveform.shape[1] / num_samples_segment)\n",
        "\n",
        "                # Split waveform into segments and save each segment to a new .wav file\n",
        "                for i in range(total_segments):\n",
        "                    start = i * num_samples_segment\n",
        "                    end = start + num_samples_segment\n",
        "                    segment = waveform[:, start:end]\n",
        "\n",
        "                    # Prepare filename for the segment\n",
        "                    segment_filename = f\"{filename.rstrip('.wav')}_segment{i}.wav\"\n",
        "                    segment_path = os.path.join(output_dir, segment_filename)\n",
        "\n",
        "                    # Save segment as a .wav file\n",
        "                    segment = (segment * 32767).short()  # Convert to 16-bit PCM format\n",
        "                    torchaudio.save(segment_path, segment, sample_rate)\n",
        "            except Exception as e:\n",
        "                  print(f\"Error processing file {audio_path}: {str(e)}\")\n",
        "    print(\"Finished Splitting files into 10 sec segments\")\n",
        "\n",
        "def extract_metadata_from_files_in_directory(directory):\n",
        "    metadata_dict = {}\n",
        "    for file in os.listdir(directory):\n",
        "        if file.endswith(\".wav\") or file.endswith('.WAV'):  # assuming you're working with wav files\n",
        "            filepath = os.path.join(directory, file)\n",
        "            metadata = mediainfo(filepath)\n",
        "\n",
        "            # Extract desired information from the comment\n",
        "            comment = metadata.get('TAG', {}).get('comment', '')\n",
        "            match = re.search(r\"Recorded at (.*) by (AudioMoth .*) at .* while .* and temperature was (.*C)\\.\", comment)\n",
        "            if match:\n",
        "                recorded_at, device_id, temperature = match.groups()\n",
        "\n",
        "                # Create a new dictionary with only the desired information\n",
        "                simplified_metadata = {\n",
        "                    'filename': file,\n",
        "                    'recorded_at': recorded_at,\n",
        "                    'device_id': device_id,\n",
        "                    'temperature': temperature,\n",
        "                }\n",
        "                metadata_dict[file] = simplified_metadata  # Store metadata using filename as key\n",
        "    return metadata_dict"
      ],
      "metadata": {
        "id": "5WfeCtVMxoRI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Enter Parameters."
      ],
      "metadata": {
        "id": "zwEHcS_5Syqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#do NOT change variable names (left of equal sign), do change variable contents (red, in quotes) for example:\n",
        "do_NOT_change = \"yes, feel free to change\"\n",
        "\n",
        "model_name = \"AST_Rana_Draytonii\"\n",
        "model_version = \"1.0\"\n",
        "\n",
        "# output will be stored in this excel file. Definitly change this one\n",
        "output_file_name = \"results.xlsx\"\n",
        "# !! If there is already a file with this name at this location it will be overwritten with the new file !!\n",
        "\n",
        "# Define the base path to where your Rana7 folder is stored in google drive\n",
        "# If youre not sure you can find it in the file explorer to the left. /content will always be the base\n",
        "base_path = '/content/drive/MyDrive/Rana_Draytonii/Rana_Draytonii_ML_Model'\n",
        "\n",
        "# This is the name of the folder within Rana_Draytionii_ML_Model where you will place the files you want reviewed\n",
        "input_folder = 'Wav_Files_Input'\n"
      ],
      "metadata": {
        "id": "9xg2gDA2grb1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**That was all you need to do for inputting data**, now just run the rest of the cells top to bottom"
      ],
      "metadata": {
        "id": "b4QnYWtgT6nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines paths for other files/directories by concatenating with the base path\n",
        "labels_path = os.path.join(base_path, 'labels.csv')\n",
        "checkpoint_path = os.path.join(base_path, 'best_audio_model1.pth')\n",
        "input_dir = os.path.join(base_path, input_folder)\n",
        "output_dir = os.path.join(base_path, 'Temp_File_Storage')\n",
        "results_path = os.path.join(base_path, 'results.xlsx')\n",
        "resampled_audio_dir = os.path.join(base_path, 'ResampledAudio')\n",
        "metadata_dict = extract_metadata_from_files_in_directory(input_dir)\n",
        "\n",
        "# Delete all files in the directory\n",
        "for filename in os.listdir(output_dir):\n",
        "    file_path = os.path.join(output_dir, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "print(\"Temp_File_Storage cleared...\")\n",
        "\n",
        "split_all_audio_files(input_dir, output_dir, segment_length_sec=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftJliwaiT3Xg",
        "outputId": "e91db727-6aa3-4467-ff51-689ba8fa78a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temp_File_Storage cleared...\n",
            "Finished Splitting files into 10 sec segments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resamples to 16 kHz and converts to mono for use with model:"
      ],
      "metadata": {
        "id": "r-ZpQd8PYSF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting...\")\n",
        "\n",
        "# Delete all files in the directory\n",
        "for filename in os.listdir(resampled_audio_dir):\n",
        "    file_path = os.path.join(resampled_audio_dir, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "print(\"ResampledAudio cleared...\")\n",
        "\n",
        "# Changes sampling frequency of audio file to 16kHz required by the AST model\n",
        "def resampler(audio_path, save_dir):\n",
        "    # load your audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # define resampler\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "\n",
        "    # resample the waveform\n",
        "    waveform_resampled = resampler(waveform)\n",
        "\n",
        "    # create new path with original filename\n",
        "    base_filename = os.path.basename(audio_path)\n",
        "    new_path = os.path.join(save_dir, base_filename)\n",
        "\n",
        "    # save the resampled audio\n",
        "    torchaudio.save(new_path, waveform_resampled, sample_rate=16000)\n",
        "\n",
        "    return new_path\n",
        "\n",
        "\n",
        "# Function to convert strero files to mono\n",
        "def stereo_to_mono(directory_path):\n",
        "  # Loop through all files in the directory\n",
        "  for filename in os.listdir(directory_path):\n",
        "      # Check if the file is a .wav file\n",
        "      if filename.endswith('.wav'):\n",
        "          # Get the full path of the file\n",
        "          file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "          # Load audio file\n",
        "          audio = AudioSegment.from_wav(file_path)\n",
        "\n",
        "          # If the audio file is stereo\n",
        "          if audio.channels == 2:\n",
        "              print(f\"Converting stereo file: {filename}\")\n",
        "\n",
        "              # Convert to mono\n",
        "              mono_audio = audio.set_channels(1)\n",
        "\n",
        "              # Replace the original file with the mono version\n",
        "              mono_audio.export(file_path, format='wav')\n",
        "\n",
        "# Define the target length for your spectrograms (only used for mel spectrogram)\n",
        "target_length = 1000\n",
        "mel_bins = 128  # Number of bins in Mel spectrogram\n",
        "\n",
        "# Convert to mono\n",
        "stereo_to_mono(output_dir)\n",
        "print(\"Files converted to Mono...\")\n",
        "\n",
        "# Process audio samples\n",
        "for filename in os.listdir(output_dir):\n",
        "    if filename.endswith('.wav') or filename.endswith('.WAV'):\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        filepath = resampler(filepath, resampled_audio_dir)  # Resample and get new file path\n",
        "print(\"Finished Preprocessing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNPGZ26czxbP",
        "outputId": "65a81d18-9b76-4a8d-909d-38fed88c27ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting...\n",
            "ResampledAudio cleared...\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment0.wav\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment1.wav\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment2.wav\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment3.wav\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment4.wav\n",
            "Converting stereo file: FAUNO-POND_20210318_223500_segment5.wav\n",
            "Files converted to Mono...\n",
            "Finished Preprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. The Big Kahuna\n",
        "\n",
        "This one will probably take a while.\n",
        "\n",
        "It will take each file and run it through the ML model to create its prediction of whether or not there is Rana Draytonii.\n",
        "\n",
        "It give its result as two predictions, the percentage likelyhood that there is a call, and the likelyhood there isnt a call. these probably wont exactly add up to 100% but will be close. the greater prediction is taken as the final prediction.\n",
        "\n",
        "It will then store the results in an excell file with the Model Name and Version, Filename, Positive score, Negative score, Prediction (positive = frog call; negative = no call), Device ID, Timestamp, and Temperature at recording.\n",
        "\n",
        "google colab has a limit for how long it can run. \"if user does not interact with his Google Colab notebook for more than 90 minutes, its instance is automatically terminated. Also, maximum lifetime of a Colab instance is 12 hours.\"\n",
        "\n",
        "so far it seems to be able to process files at about 60-80% of the time of the audio files themselves. that just an estimate from my tests but that would mean you could put an hour and a half worth of .wav files in and walk away and it should finish with no problem before timing out. Thi is also assuming your computer doesnt go to sleep(maybe make sure the sleep time on your computer is set to never before walking away from a big batch).\n",
        "\n",
        "In the future this could probably be improved by running this Jupyter Notebook in a software like Anaconda instead of Google Colab, which would run it on your personal machine rather than googles cloud. This could give more power (also allowing the script to leverage gpu or multiple cpu cores in order to run faster) but comes at the cost of more memory usage on your machine."
      ],
      "metadata": {
        "id": "RL_UAkOygrzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hhoZG9YzpQh",
        "outputId": "7800e3a4-63b2-41d7-cc79-dbbb1585f2dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting...\n",
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: False, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=1188\n",
            "[*INFO] load checkpoint: /content/drive/MyDrive/Rana_Draytonii/Rana_Draytonii_ML_Model/best_audio_model1.pth\n",
            "Model Loaded...\n",
            "Found 30 segments\n",
            "Using model AST_Rana_Draytonii version 1.0 to make predictions on 30 audio files\n",
            "---------------Start Predictions---------------\n",
            "Processed 30/30 files...\n",
            "---------------Finished Predictions---------------\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting...\")\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "class RanaDraytoniiDataset(Dataset):\n",
        "    def __init__(self, dir_path, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "        # List all files in the directory\n",
        "        self.files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "        print(f\"Found {len(self.files)} segments\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_file = self.files[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            audio_data = self.transform(audio_file)\n",
        "            return audio_data\n",
        "        else:\n",
        "            return audio_file\n",
        "\n",
        "def make_features(waveform, mel_bins, target_length=1000):\n",
        "    sr = 16000  # the sample rate is always 16kHz\n",
        "\n",
        "    fbank = torchaudio.compliance.kaldi.fbank(\n",
        "        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
        "        window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10)\n",
        "\n",
        "    n_frames = fbank.shape[0]\n",
        "\n",
        "    p = target_length - n_frames\n",
        "    if p > 0:\n",
        "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "        fbank = m(fbank)\n",
        "    elif p < 0:\n",
        "        fbank = fbank[0:target_length, :]\n",
        "\n",
        "    fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
        "    return fbank\n",
        "\n",
        "def make_features_fixed(wav_name):\n",
        "    waveform, sr = torchaudio.load(wav_name)\n",
        "    return make_features(waveform, mel_bins=128)\n",
        "\n",
        "def load_label(label_csv):\n",
        "    with open(label_csv, 'r') as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        next(reader, None)  # skip the headers\n",
        "        labels = {line[0]: line[1] for line in reader}\n",
        "    return labels\n",
        "\n",
        "# Load your labels\n",
        "labels = load_label(labels_path)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = ASTModel(\n",
        "    label_dim=2,\n",
        "    fstride=10,\n",
        "    tstride=10,\n",
        "    input_fdim=128,\n",
        "    input_tdim=1000,\n",
        "    imagenet_pretrain=False,\n",
        "    audioset_pretrain=False,\n",
        "    model_size='base384'\n",
        ")\n",
        "\n",
        "# Load your fine-tuned weights\n",
        "print(f'[*INFO] load checkpoint: {checkpoint_path}')\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "audio_model = torch.nn.DataParallel(model, device_ids=[0])\n",
        "audio_model.load_state_dict(checkpoint)\n",
        "audio_model = audio_model.to(torch.device(\"cpu\"))\n",
        "audio_model.eval()\n",
        "print(\"Model Loaded...\")\n",
        "\n",
        "# Create a Dataset for your audio files\n",
        "audio_files_dataset = RanaDraytoniiDataset(resampled_audio_dir, transform=make_features_fixed)\n",
        "\n",
        "# Create a DataLoader for your audio files\n",
        "audio_files_data_loader = DataLoader(audio_files_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Create an empty DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['Model Name : Version', 'File Name', 'Prediction', 'Times Heard', 'Device ID', 'Timestamp', 'Temperature', 'Review Date'])\n",
        "print('Using model ' + model_name + ' version ' + model_version + ' to make predictions on ' + str(len(audio_files_dataset)) + ' audio files')\n",
        "print(\"---------------Start Predictions---------------\")\n",
        "\n",
        "# Add a variable to keep track of the total number of processed files\n",
        "total_processed_files = 0\n",
        "\n",
        "metadata_dict = {md['filename']: md for md in metadata_dict.values()}\n",
        "\n",
        "# Initialize a dictionary to store the predictions for each original audio file\n",
        "file_predictions = defaultdict(list)\n",
        "\n",
        "# Iterate over the audio files\n",
        "for i, batch in enumerate(audio_files_data_loader):\n",
        "    # Only apply unsqueeze(1) if waveform is 2D\n",
        "    if len(batch.shape) == 2:\n",
        "        batch = batch.unsqueeze(1)\n",
        "\n",
        "    # Move the data to the correct device\n",
        "    feats_data = batch.to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Make the prediction\n",
        "    with torch.no_grad():\n",
        "        output = audio_model.forward(feats_data)\n",
        "        output = torch.sigmoid(output)\n",
        "    result_output = output.data.cpu().numpy()\n",
        "\n",
        "    # Check each segment for frog call\n",
        "    for j, file_result in enumerate(result_output):\n",
        "        try:\n",
        "            # Get the file name using the total number of processed files\n",
        "            file_name = os.path.basename(audio_files_dataset.files[total_processed_files + j])\n",
        "\n",
        "            # Remove the segment part and the extension from the file name\n",
        "            base_file_name, _ = os.path.splitext(file_name.split('_segment')[0])\n",
        "\n",
        "            # Get the scores\n",
        "            positive_score = file_result[0]\n",
        "            negative_score = file_result[1]\n",
        "\n",
        "            # Get the prediction\n",
        "            if positive_score > negative_score:\n",
        "                file_predictions[base_file_name].append(\"Positive\")\n",
        "            else:\n",
        "                file_predictions[base_file_name].append(\"Negative\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {audio_files_dataset.files[total_processed_files + j]}: {e}\")\n",
        "\n",
        "\n",
        "    # Update the total number of processed files\n",
        "    total_processed_files += len(batch)\n",
        "    print(f\"Processed {total_processed_files}/{str(len(audio_files_dataset))} files...\")\n",
        "print(\"---------------Finished Predictions---------------\")\n",
        "\n",
        "# Group the segments into consecutive ranges\n",
        "def group_consecutive_elements(data):\n",
        "    ranges = []\n",
        "    for k, g in itertools.groupby(enumerate(data), lambda ix: ix[0] - ix[1]):\n",
        "        consecutive_elements = list(map(lambda x: x[1], g))\n",
        "        ranges.append((consecutive_elements[0], consecutive_elements[-1]))\n",
        "    return ranges\n",
        "\n",
        "# Iterate over the predictions and store the results\n",
        "for base_file_name, predictions in file_predictions.items():\n",
        "    heard_segments = [i for i, pred in enumerate(predictions) if pred == \"Positive\"]\n",
        "\n",
        "    if len(heard_segments) == 0:\n",
        "        times_str = 'N/A'\n",
        "    else:\n",
        "        heard_ranges = group_consecutive_elements(heard_segments)\n",
        "        times_str = ', '.join(f'{s*10}-{(e+1)*10}' for s, e in heard_ranges)\n",
        "\n",
        "    # Get the prediction\n",
        "    if times_str == 'N/A':\n",
        "        prediction = 'Negative'\n",
        "    else:\n",
        "        prediction = 'Positive!'\n",
        "\n",
        "    # Try to get metadata for both .WAV and .wav versions of the file\n",
        "    metadata = metadata_dict.get(base_file_name + '.WAV', metadata_dict.get(base_file_name + '.wav', {}))\n",
        "    device_id = metadata.get('device_id')\n",
        "    recorded_at = metadata.get('recorded_at')\n",
        "    temperature = metadata.get('temperature')\n",
        "\n",
        "    new_row = pd.DataFrame({\n",
        "        'Model Name : Version' : [model_name + ':' + model_version],\n",
        "        'File Name': [base_file_name],\n",
        "        'Prediction': [prediction],\n",
        "        'Times Heard': [times_str],  # Add this column\n",
        "        'Device ID': [device_id],\n",
        "        'Timestamp': [recorded_at],\n",
        "        'Temperature': [temperature],\n",
        "        'Review Date': [datetime.now().strftime('%Y-%m-%d')]  # Add current date\n",
        "    })\n",
        "\n",
        "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel(results_path, index=False)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}